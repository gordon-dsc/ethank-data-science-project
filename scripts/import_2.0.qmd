---
title: "Data Import Script"
format: html
---

# Libraries and Packages

Install needed packages for data import and collection

```{r packages}
install.packages("tidyverse")
install.packages("rvest")
install.packages("chromote")
library(tidyverse)
library(rvest)
library(chromote)
```

# Data Collection

Data for this project will consist of all headlines from the Wall Street Journal over the last ten years (January 1, 2015 through December 31, 2024). The resulting data set will have variables for title, date publication, journal column, and publishing time. The intent is to web scrape the WSJ archives (<https://www.wsj.com/news/archive/years>) by cycling through every year, month and day in the target range.

To evaluate how news sentiment effects the markets daily stock market prices and valuations based on the S&P500 will be obtained from a variety of websites.

## Web-scrapping

### WSJ Scrapping

The first web scrapping task will create a data set from the Wall Street Journal archives

First code block will generate all daily archive URLs from the time period

```{r}
#url for Wall Street Journal archives
wsj_url <- "https://www.wsj.com/news/archive/years"

#get list of all links to archive months
read_html(wsj_url)|>
  html_elements(".WSJTheme--month-link--1N8tTFWa") |>
  html_attr("href") |>
  enframe(name = NULL, value = "month") |>
  filter(str_detect(month, 
                    pattern = paste(c(as.character(c(2015:2024))), collapse = "|"))) |>
  mutate(full_urls = paste("https://www.wsj.com", month, sep = "")) ->
  month_urls

x <- 1
daily_urls <- tibble(full_urls = character())
while (x <= length(month_urls$full_urls)) {
  #add urls of days to data set
  read_html(month_urls$full_urls[x]) |>
    html_elements(".WSJTheme--day-link--19pByDpZ") |>
    html_attr("href") |>
    enframe(name = NULL, value = "day") |>
    mutate(full_daily_urls = paste("https://www.wsj.com", day, sep = "")) ->
    temp_data
  
  #add daily urls from month to final tibble
  y <- 1
  while(y <= length(temp_data$full_daily_urls)){
      add_row(daily_urls, full_urls = temp_data$full_daily_urls[y]) ->
      daily_urls
      y <- y +1
  }
  x <- x + 1
}

#make a tibble
tibble(daily_urls) ->
  daily_urls

#remove loop items from environment
rm(x)
rm(y)
rm(temp_data)
rm(month_urls)
```

split data up into ten blocks to make scrape faster

```{r}
#split data up into ten blocks for scrapping to avoid time out error
daily_urls |>
  slice(1:365) ->
  daily_urls_1

daily_urls |>
  slice(366:730) ->
  daily_urls_2

daily_urls |>
  slice(731:1095) ->
  daily_urls_3

daily_urls |>
  slice(1096:1460) ->
  daily_urls_4

daily_urls |>
  slice(1461:1825) ->
  daily_urls_5

daily_urls |>
  slice(1826:2190) ->
  daily_urls_6

daily_urls |>
  slice(2191:2555) ->
  daily_urls_7

daily_urls |>
  slice(2556:2920) ->
  daily_urls_8

daily_urls |>
  slice(2921:3285) ->
  daily_urls_9

daily_urls |>
  slice(3286:3653) ->
  daily_urls_10

daily_urls_vector <- c(daily_urls_1,
                       daily_urls_2,
                       daily_urls_3,
                       daily_urls_4,
                       daily_urls_6,
                       daily_urls_7,
                       daily_urls_8,
                       daily_urls_9,
                       daily_urls_10)
```

Next code block visits each daily archive URL in period and scraps title, date of publication, journal column, and publishing time. Run this chuck on each of the daily url blocks changing the reference in line 140 and 142 for each

```{r}
#loop to run the code chunck on each of the daily urls data frames
z <- 1 #variable to control loop
while(x <= length(daily_urls_vector)){

    #open headless browser tab and navigate to WSJ archive page
    #MUST HAVE CHROME RUNNING ON COMPUTER DURING EXECUTION (at least in my testing)
    options(chromote.headless = "new") #set option to use new headless mode
    brow <- ChromoteSession$new()
    #brow$view()
    
    
    x <- 1
    full_wsj_data <- tibble(
      headlines = character(),
      column = character(),
      pub_time = character(),
      url = character()
    )
    while(x <= length(daily_urls_9$full_urls)){
    #check to see if there is a next page button present
    url_temp = daily_urls_9$full_urls[x]
    print(x)
    
    repeat{
      #scrape title
      read_html(url_temp)|>
        html_elements(".WSJTheme--headlineText--He1ANr9C ")|>
        html_text2() -> 
          headlines
      
      #scrape column name
      read_html(url_temp) |>
        html_elements(".WSJTheme--articleType--34Gt-vdG") |>
        html_text2() ->
        column
      
      #scrape publishing time
      read_html(url_temp) |>
        html_elements(".WSJTheme--timestamp--22sfkNDv") |>
        html_text2() ->
        pub_time
      
      #open new tab
      brow_temp <- brow$new_session()
      brow_temp$Page$navigate(url_temp) #navigate to tab
      Sys.sleep(1) #wait for website to load
      #retrive bottom panel HTML context
      node <- brow_temp$DOM$querySelector(
        nodeId = brow_temp$DOM$getDocument()$root$nodeId,
        selector = ".WSJTheme--SimplePaginator__right--2syX0g5l"
      )
      
      #if next page button exits change url to next page else advance to next day
      if(node$nodeId > 0) {
        #get outerHTML of the node
        html_content <- brow_temp$DOM$getOuterHTML(nodeId = node$nodeId)
        html_content$outerHTML |>
          minimal_html() |>
          html_elements("a") |>
          html_attr("href") -> url_end
      
        #add data to full_wsj_data
        y <- 1
        while(y <= length(headlines)){
          add_row(full_wsj_data, 
                  headlines = headlines[y],
                  column = column[y],
                  pub_time = pub_time[y],
                  url = rep(url_temp, length(headlines))) ->
          full_wsj_data
          y <- y +1
      }
        
        #update temp url
        url_temp <- paste("https://www.wsj.com", url_end, sep = "")
        
        #close tab
        brow_temp$close()
        rm(brow_temp)
        
        } else {
          #add data to full_wsj_data
          y <- 1
          while(y <= length(headlines)){
            add_row(full_wsj_data, 
                    headlines = headlines[y],
                    column = column[y],
                    pub_time = pub_time[y],
                    url = rep(url_temp, length(headlines))) ->
            full_wsj_data
            y <- y +1
        }
    
          #close tab
          brow_temp$close()
          rm(brow_temp)
        }
    
      if(node$nodeId < 1){break}
    }
    
      x <- x + 1
    }
    
    
    #close current tab
    brow$close()
    
    #shut down browser
    brow$parent$close()
    
    #write data to csv in imported data file
    getwd()
    write_csv(x = full_wsj_data,
              file = paste("./data/imported_data/wsj_data_", z, ".csv", sep = ""))
}
```

Combine all data frames into one frame

```{r}
#read in data frames
read_csv("../data/imported_data/wsj_data_1.csv") -> wsj_data_1
read_csv("../data/imported_data/wsj_data_2.csv") -> wsj_data_2
read_csv("../data/imported_data/wsj_data_3.csv") -> wsj_data_3
read_csv("../data/imported_data/wsj_data_4.csv") -> wsj_data_4
read_csv("../data/imported_data/wsj_data_5.csv") -> wsj_data_5
read_csv("../data/imported_data/wsj_data_6.csv") -> wsj_data_6
read_csv("../data/imported_data/wsj_data_7.csv") -> wsj_data_7
read_csv("../data/imported_data/wsj_data_8.csv") -> wsj_data_8
read_csv("../data/imported_data/wsj_data_9.csv") -> wsj_data_9
read_csv("../data/imported_data/wsj_data_10.csv") -> wsj_data_10

#combine into data frame
full_wsj_data <- wsj_data_1

#add table two
add_row(full_wsj_data,
          headlines = wsj_data_2$headlines,
          column = wsj_data_2$column,
          pub_time = wsj_data_2$pub_time,
          url = wsj_data_2$url) -> full_wsj_data

#add table three
add_row(full_wsj_data,
          headlines = wsj_data_3$headlines,
          column = wsj_data_3$column,
          pub_time = wsj_data_3$pub_time,
          url = wsj_data_3$url) -> full_wsj_data

#add table four
add_row(full_wsj_data,
          headlines = wsj_data_4$headlines,
          column = wsj_data_4$column,
          pub_time = wsj_data_4$pub_time,
          url = wsj_data_4$url) -> full_wsj_data

#add table five
add_row(full_wsj_data,
          headlines = wsj_data_5$headlines,
          column = wsj_data_5$column,
          pub_time = wsj_data_5$pub_time,
          url = wsj_data_5$url) -> full_wsj_data

#add table six
add_row(full_wsj_data,
          headlines = wsj_data_6$headlines,
          column = wsj_data_6$column,
          pub_time = wsj_data_6$pub_time,
          url = wsj_data_6$url) -> full_wsj_data

#add table seven
add_row(full_wsj_data,
          headlines = wsj_data_7$headlines,
          column = wsj_data_7$column,
          pub_time = wsj_data_7$pub_time,
          url = wsj_data_7$url) -> full_wsj_data

#add table eight
add_row(full_wsj_data,
          headlines = wsj_data_8$headlines,
          column = wsj_data_8$column,
          pub_time = wsj_data_8$pub_time,
          url = wsj_data_8$url) -> full_wsj_data

#add table nine
add_row(full_wsj_data,
          headlines = wsj_data_9$headlines,
          column = wsj_data_9$column,
          pub_time = wsj_data_9$pub_time,
          url = wsj_data_9$url) -> full_wsj_data

#add table ten
add_row(full_wsj_data,
          headlines = wsj_data_10$headlines,
          column = wsj_data_10$column,
          pub_time = wsj_data_10$pub_time,
          url = wsj_data_10$url) -> full_wsj_data

write_csv(x = full_wsj_data,
            file = "../data/imported_data/full_wsj_data.csv")
```

### Stock Data Scrapping

Will obtain S&P 500 data (\^GSPC) from yahoo finance over the time period

```{r}
yahoo_fin_url <- "https://finance.yahoo.com/quote/%5EGSPC/history/?period1=1388534400&period2=1735603200"

read_html(yahoo_fin_url) |>
  html_elements(".talbe-container yf-1jecxey") |>
  html_table()
```
