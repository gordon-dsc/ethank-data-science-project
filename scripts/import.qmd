---
title: "Data Import Script"
format: html
---

# Libraries and Packages

Install needed packages for data import and collection

```{r packages}
install.packages("tidyverse")
install.packages("rvest")
install.packages("chromote")
library(tidyverse)
library(rvest)
library(chromote)
```

# Data Collection

Data for this project will consist of all headlines from the Wall Street Journal over the last ten years (January 1, 2015 through December 31, 2024). The resulting data set will have variables for title, date publication, journal column, and publishing time. The intent is to web scrape the WSJ archives (<https://www.wsj.com/news/archive/years>) by cycling through every year, month and day in the target range.

To evaluate how news sentiment effects the markets daily stock market prices and valuations based on the S&P500 will be obtained from a variety of websites.

## Web-scrapping

### WSJ Scrapping

The first web scrapping task will create a data set from the Wall Street Journal archives

First code block will generate all daily archive URLs from the time period

```{r}
#url for Wall Street Journal archives
wsj_url <- "https://www.wsj.com/news/archive/years"

#get list of all links to archive months
read_html(wsj_url)|>
  html_elements(".WSJTheme--month-link--1N8tTFWa") |>
  html_attr("href") |>
  enframe(name = NULL, value = "month") |>
  filter(str_detect(month, 
                    pattern = paste(c(as.character(c(2015:2024))), collapse = "|"))) |>
  mutate(full_urls = paste("https://www.wsj.com", month, sep = "")) ->
  month_urls

x <- 1
daily_urls <- tibble(full_urls = character())
while (x <= length(month_urls$full_urls)) {
  #add urls of days to data set
  read_html(month_urls$full_urls[x]) |>
    html_elements(".WSJTheme--day-link--19pByDpZ") |>
    html_attr("href") |>
    enframe(name = NULL, value = "day") |>
    mutate(full_daily_urls = paste("https://www.wsj.com", day, sep = "")) ->
    temp_data
  
  #add daily urls from month to final tibble
  y <- 1
  while(y <= length(temp_data$full_daily_urls)){
      add_row(daily_urls, full_urls = temp_data$full_daily_urls[y]) ->
      daily_urls
      y <- y +1
  }
  x <- x + 1
}

#make a tibble
tibble(daily_urls) ->
  daily_urls

#remove loop items from environment
rm(x)
rm(y)
rm(temp_data)
```

Next code block visits each daily archive URL in period and scraps title, date of publication, journal column, and publishing time

```{r}
#open headless browser tab and navigate to WSJ archive page
#MUST HAVE CHROME RUNNING ON COMPUTER DURING EXECUTION (at least in my testing)
options(chromote.headless = "new") #set option to use new headless mode
brow <- ChromoteSession$new()
brow$view()

brow$Page$navigate(daily_urls$full_urls[2])
brow$Page$loadEventFired() #pasues R functions until page loads

x <- 1
while(x <= length(daily_urls$full_urls[x])){
#check to see if there is a next page button present

#scrape title
read_html(daily_urls$full_urls[1])|>
  html_elements(".WSJTheme--headlineText--He1ANr9C ")|>
  html_text2() ->
  headlines

#scrape column name
read_html(daily_urls$full_urls[1]) |>
  html_elements(".WSJTheme--articleType--34Gt-vdG") |>
  html_text2() ->
  column

#scrape publishing time
read_html(daily_urls$full_urls[1]) |>
  html_elements(".WSJTheme--timestamp--22sfkNDv") |>
  html_text2() ->
  pub_time


x <- x + 1
}

#find a way to add this into the loop
tibble(headlines,
       column,
       pub_time,
       url = rep(daily_urls$full_urls[1], length(headlines))) -> 
  wsj_data


#close current tab
brow$close()

#shut down browser
brow$parent$close()
```

### Stock Data Scrapping

Will obtain S&P 500 data (\^GSPC) from yahoo finance over the time period

```{r}
yahoo_fin_url <- "https://finance.yahoo.com/quote/%5EGSPC/history/?period1=1388534400&period2=1735603200"

read_html(yahoo_fin_url) |>
  html_elements(".talbe-container yf-1jecxey") |>
  html_table()
```
