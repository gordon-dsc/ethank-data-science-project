---
title: "Data Import Script"
format: html
---

# Libraries and Packages

Install needed packages for data import and collection

```{r packages}
install.packages("tidyverse")
install.packages("rvest")
install.packages("chromote")
library(tidyverse)
library(rvest)
library(chromote)
```

# Data Collection

Data for this project will consist of all headlines from the Wall Street Journal over the last ten years (January 1, 2015 through December 31, 2024). The resulting data set will have variables for title, date publication, journal column, and publishing time. The intent is to web scrape the WSJ archives (<https://www.wsj.com/news/archive/years>) by cycling through every year, month and day in the target range.

To evaluate how news sentiment effects the markets daily stock market prices and valuations based on the S&P500 will be obtained from a variety of websites.

## Web-scrapping

### WSJ Scrapping

The first web scrapping task will create a data set from the Wall Street Journal archives

First code block will generate all daily archive URLs from the time period

```{r}
#url for Wall Street Journal archives
wsj_url <- "https://www.wsj.com/news/archive/years"

#get list of all links to archive months
read_html(wsj_url)|>
  html_elements(".WSJTheme--month-link--1N8tTFWa") |>
  html_attr("href") |>
  enframe(name = NULL, value = "month") |>
  filter(str_detect(month, 
                    pattern = paste(c(as.character(c(2015:2024))), collapse = "|"))) |>
  mutate(full_urls = paste("https://www.wsj.com", month, sep = "")) ->
  month_urls

x <- 1
daily_urls <- tibble(full_urls = character())
while (x <= length(month_urls$full_urls)) {
  #add urls of days to data set
  read_html(month_urls$full_urls[x]) |>
    html_elements(".WSJTheme--day-link--19pByDpZ") |>
    html_attr("href") |>
    enframe(name = NULL, value = "day") |>
    mutate(full_daily_urls = paste("https://www.wsj.com", day, sep = "")) ->
    temp_data
  
  #add daily urls from month to final tibble
  y <- 1
  while(y <= length(temp_data$full_daily_urls)){
      add_row(daily_urls, full_urls = temp_data$full_daily_urls[y]) ->
      daily_urls
      y <- y +1
  }
  x <- x + 1
}

#make a tibble
tibble(daily_urls) ->
  daily_urls

#remove loop items from environment
rm(x)
rm(y)
rm(temp_data)
```

split data up into ten blocks to make scrape faster

```{r}
#split data up into ten blocks to make scrape faster
daily_urls |>
  slice(1:365) ->
  daily_urls_1

daily_urls |>
  slice(366:730) ->
  daily_urls_2

daily_urls |>
  slice(731:1095) ->
  daily_urls_3

daily_urls |>
  slice(1096:1460) ->
  daily_urls_4

daily_urls |>
  slice(1461:1825) ->
  daily_urls_5

daily_urls |>
  slice(1826:2190) ->
  daily_urls_6

daily_urls |>
  slice(2191:2555) ->
  daily_urls_7

daily_urls |>
  slice(2556:2920) ->
  daily_urls_8

daily_urls |>
  slice(2921:3285) ->
  daily_urls_9

daily_urls |>
  slice(3286:3653) ->
  daily_urls_10
```

Next code block visits each daily archive URL in period and scraps title, date of publication, journal column, and publishing time. Run this chuck on each of the daily url blocks changing the reference in line 140 and 142 for each

```{r}
#open headless browser tab and navigate to WSJ archive page
#MUST HAVE CHROME RUNNING ON COMPUTER DURING EXECUTION (at least in my testing)
options(chromote.headless = "new") #set option to use new headless mode
brow <- ChromoteSession$new()
#brow$view()


x <- 1
full_wsj_data <- tibble(
  headlines = character(),
  column = character(),
  pub_time = character(),
  url = character()
)
while(x <= length(daily_urls_10$full_urls)){
#check to see if there is a next page button present
url_temp = daily_urls_10$full_urls[x]
print(x)

repeat{
  #scrape title
  read_html(url_temp)|>
    html_elements(".WSJTheme--headlineText--He1ANr9C ")|>
    html_text2() -> 
      headlines
  
  #scrape column name
  read_html(url_temp) |>
    html_elements(".WSJTheme--articleType--34Gt-vdG") |>
    html_text2() ->
    column
  
  #scrape publishing time
  read_html(url_temp) |>
    html_elements(".WSJTheme--timestamp--22sfkNDv") |>
    html_text2() ->
    pub_time
  
  #open new tab
  brow_temp <- brow$new_session()
  brow_temp$Page$navigate(url_temp) #navigate to tab
  Sys.sleep(1) #wait for website to load
  #retrive bottom panel HTML context
  node <- brow_temp$DOM$querySelector(
    nodeId = brow_temp$DOM$getDocument()$root$nodeId,
    selector = ".WSJTheme--SimplePaginator__right--2syX0g5l"
  )
  
  #if next page button exits change url to next page else advance to next day
  if(node$nodeId > 0) {
    #get outerHTML of the node
    html_content <- brow_temp$DOM$getOuterHTML(nodeId = node$nodeId)
    html_content$outerHTML |>
      minimal_html() |>
      html_elements("a") |>
      html_attr("href") -> url_end
  
    #add data to full_wsj_data
    y <- 1
    while(y <= length(headlines)){
      add_row(full_wsj_data, 
              headlines = headlines[y],
              column = column[y],
              pub_time = pub_time[y],
              url = rep(url_temp, length(headlines))) ->
      full_wsj_data
      y <- y +1
  }
    
    #update temp url
    url_temp <- paste("https://www.wsj.com", url_end, sep = "")
    
    #close tab
    brow_temp$close()
    rm(brow_temp)
    
    } else {
      #add data to full_wsj_data
      y <- 1
      while(y <= length(headlines)){
        add_row(full_wsj_data, 
                headlines = headlines[y],
                column = column[y],
                pub_time = pub_time[y],
                url = rep(url_temp, length(headlines))) ->
        full_wsj_data
        y <- y +1
    }

      #close tab
      brow_temp$close()
      rm(brow_temp)
    }

  if(node$nodeId < 1){break}
}

  x <- x + 1
}


#close current tab
brow$close()

#shut down browser
brow$parent$close()

#write data to csv in imported data file
getwd()
write_csv(x = full_wsj_data,
          file = "./data/imported_data/wsj_data_10.csv")
```

### Stock Data Scrapping

Will obtain S&P 500 data (\^GSPC) from yahoo finance over the time period

```{r}
yahoo_fin_url <- "https://finance.yahoo.com/quote/%5EGSPC/history/?period1=1388534400&period2=1735603200"

#attempt to read the table from the html page
read_html(yahoo_fin_url) |>
  html_elements(".talbe-container yf-1jecxey") |>
  html_table

read_html(yahoo_fin_url) |>
  html_elements(".yf-1jecxey") |>
  html_text2()
```

```{r}
full_wsj_data ->
  full_wsj_data_7a

daily_urls_7 |>
  slice(224:365) ->
  daily_urls_7b

add_row(f,
        headlines = full_wsj_data$headlines,
        column = full_wsj_data$column,
        pub_time = full_wsj_data$pub_time,
        url = full_wsj_data$url) -> t
```
